{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2510e41a-7dc8-400f-9e41-745bd4fbbc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01818181818181818, 0.05454545454545454, 0.2, 1.0, 0.36363636363636365, 0.6, 0.14545454545454545, 0.0]\n"
     ]
    }
   ],
   "source": [
    "## Q1) Min Max Scaling is a feature scaling method which is used to normalize the data-points within the range[0,1].\n",
    "#      It is given by the formula x_scaled=(x_i-x_min)/(x_max-x_min).\n",
    "x_i=[2,4,12,56,21,34,9,1]\n",
    "x_max=max(x_i)\n",
    "x_min=min(x_i)\n",
    "\n",
    "x_scaled=[]\n",
    "for i in x_i:\n",
    "    x_scaled.append((i-x_min)/(x_max-x_min))\n",
    "\n",
    "print(x_scaled)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "36c6e581-3211-44f2-aa08-b633d78c9259",
   "metadata": {},
   "source": [
    "Q2) In Unit vector method of feature scaling the vector components get divided by the magnitude of the vector thus \n",
    "    the sum of the square of the new components is always 1, whereas in min max scaling the range is between [0,1]."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4144d58-43d7-47bc-b562-be8b35ced7b5",
   "metadata": {},
   "source": [
    "Q3) Principle Componenet Analysis is used to reduce higher dimensional features to lower dimensional feature.\n",
    "    For example if we have two features F1 and F2, with PCA we can plot a best fit line and then take the \n",
    "    data-points with respect to the new line. PCA reduces the data loss which can be caused by feature extraction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "188d78d7-fbcf-456f-884b-f9a8ccbf9f43",
   "metadata": {},
   "source": [
    "Q4) In Feature Extraction we completely ignore some features whereas in PCA that is taken care of."
   ]
  },
  {
   "cell_type": "raw",
   "id": "caf5ae37-4f5e-40d2-80d9-9a67bd568ae6",
   "metadata": {},
   "source": [
    "Q5) The features of price, rating and delivery time has completely different units and magnitudes hence\n",
    "    requires scaling down within suitable range.This can be done by normalizing the datapoints using\n",
    "    Max Min Scaler."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b9637d5-5073-4c70-9b74-f8e3312a21cb",
   "metadata": {},
   "source": [
    "Q6) We would use PCA for getting the best-fit curve/plane(or higher dimension) and then transform the data-points\n",
    "    for futher analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "438a0767-65c4-46b1-93a3-c9c3675e9b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "## Q7)\n",
    "x_i=[1,5,10,15,20]\n",
    "x_max=max(x_i)\n",
    "x_min=min(x_i)\n",
    "\n",
    "x_scaled=[]\n",
    "for i in x_i:\n",
    "    x_std_i=(i-x_min)/(x_max-x_min)\n",
    "    x_scaled.append(x_std_i*2-1)\n",
    "\n",
    "print(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aeced6-38fd-4548-a57f-ac5851ca61e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
